# Wrapper-Only API Example
#
# This example demonstrates using the generated wrapper modules:
#
# 1. Generated wrappers (Vllm.LLM, Vllm.SamplingParams, etc.) for type-safe bindings
# 2. Runtime attribute access for Python refs returned by vLLM
#
# Run: mix run examples/direct_api.exs

IO.puts("=== Wrapper-Only API Example ===\n")

Snakepit.run_as_script(fn ->
  get_attr = fn ref, attr ->
    case SnakeBridge.Runtime.get_attr(ref, attr) do
      {:ok, value} -> value
      {:error, reason} -> raise "Failed to read #{attr}: #{inspect(reason)}"
    end
  end

  # ============================================================================
  # Part 1: Generated Wrappers (Type-Safe Bindings)
  # ============================================================================
  # The Vllm.* modules are generated by SnakeBridge and provide type-safe,
  # documented bindings.

  IO.puts("--- Part 1: Generated Wrappers (Type-Safe Bindings) ---\n")

  {:ok, llm} = Vllm.LLM.new("facebook/opt-125m")
  llm_ref = SnakeBridge.Ref.from_wire_format(llm)

  runtime_opts =
    case llm_ref.pool_name do
      nil -> [session_id: llm_ref.session_id]
      pool_name -> [session_id: llm_ref.session_id, pool_name: pool_name]
    end

  {:ok, params} =
    Vllm.SamplingParams.new([], temperature: 0.8, max_tokens: 50, __runtime__: runtime_opts)

  IO.puts("Created LLM via Vllm.LLM.new/2")
  IO.puts("Created SamplingParams via Vllm.SamplingParams.new/2")

  # Generate text
  {:ok, outputs} =
    Vllm.LLM.generate(llm, ["The future of AI is"], [],
      sampling_params: params,
      __runtime__: runtime_opts
    )

  output = Enum.at(outputs, 0)
  prompt = get_attr.(output, :prompt)
  completions = get_attr.(output, :outputs)
  first = Enum.at(completions, 0)
  text = get_attr.(first, :text)

  IO.puts("\nPrompt: #{prompt}")
  IO.puts("Generated: #{text}")

  # ============================================================================
  # Part 2: Wrapper Attributes and Helpers
  # ============================================================================
  # Note: We reuse the LLM from Part 1 since vLLM can only have one LLM instance
  # per GPU at a time.

  IO.puts("\n--- Part 2: Wrapper Attributes and Helpers ---\n")

  # Create SamplingParams using generated wrapper directly
  {:ok, params2} =
    Vllm.SamplingParams.new([], temperature: 0.7, max_tokens: 30, __runtime__: runtime_opts)

  IO.puts("Created SamplingParams via Vllm.SamplingParams.new/2")

  # Reuse the LLM from Part 1 (only one LLM instance can exist per GPU)
  IO.puts("Reusing LLM from Part 1 (vLLM allows one LLM instance per GPU)")

  # Generate using wrapper's method with the existing LLM
  {:ok, outputs2} =
    Vllm.LLM.generate(llm, ["Hello from generated wrappers!"], [],
      sampling_params: params2,
      __runtime__: runtime_opts
    )

  output2 = Enum.at(outputs2, 0)
  text2 = get_attr.(output2, :outputs) |> Enum.at(0) |> then(&get_attr.(&1, :text))
  IO.puts("Generated: #{text2}")

  # Access attributes via generated accessors (where available)
  {:ok, temp} = Vllm.SamplingParams.temperature(params2)
  {:ok, max_tok} = Vllm.SamplingParams.max_tokens(params2)
  IO.puts("\nSamplingParams.temperature: #{temp}")
  IO.puts("SamplingParams.max_tokens: #{max_tok}")

  # ============================================================================
  # Part 3: Library Metadata
  # ============================================================================
  # Use generated wrappers for library metadata.

  IO.puts("\n--- Part 3: Library Metadata ---\n")

  case Vllm.CollectEnv.get_vllm_version() do
    {:ok, version} -> IO.puts("vLLM version: #{version}")
    {:error, reason} -> IO.puts("vLLM version lookup failed: #{inspect(reason)}")
  end

  # ============================================================================
  # Part 4: Type Checking and Error Handling
  # ============================================================================

  IO.puts("\n--- Part 4: Type Checking and Error Handling ---\n")

  # Type checking
  is_ref = SnakeBridge.Ref.ref?(llm)
  IO.puts("Is llm a Python reference? #{is_ref}")

  is_ref_list = SnakeBridge.Ref.ref?([1, 2, 3])
  IO.puts("Is [1,2,3] a Python reference? #{is_ref_list}")

  # Error handling with non-bang versions
  # Note: We demonstrate with SamplingParams since only one LLM can exist per GPU
  IO.puts("\nNote: Vllm.LLM.new/2 returns {:ok, llm} or {:error, reason}")
  IO.puts("(Skipping demo since only one LLM instance allowed per GPU)")

  case Vllm.SamplingParams.new([], temperature: 0.5, __runtime__: runtime_opts) do
    {:ok, _} -> IO.puts("Vllm.SamplingParams.new/2 returns {:ok, params} on success")
    {:error, reason} -> IO.puts("Error: #{inspect(reason)}")
  end

  IO.puts("\nWrapper-only API example complete!")
end)
