# Generated by SnakeBridge v0.15.1 - DO NOT EDIT MANUALLY
# Regenerate with: mix compile
# Library: vllm 0.14.0
# Python module: vllm
# Python class: LLMEngine

defmodule Vllm.LLMEngine do
  @moduledoc """
  Legacy LLMEngine for backwards compatibility.
  """
  def __snakebridge_python_name__, do: "vllm"
  def __snakebridge_python_class__, do: "LLMEngine"
  def __snakebridge_library__, do: "vllm"
  @opaque t :: SnakeBridge.Ref.t()

  @doc """
  Initialize self.  See help(type(self)) for accurate signature.

  ## Parameters

  - `vllm_config` (term())
  - `executor_class` (term())
  - `log_stats` (boolean())
  - `aggregate_engine_logging` (boolean() default: False)
  - `usage_context` (term() default: <UsageContext.ENGINE_CONTEXT: 'ENGINE_CONTEXT'>)
  - `stat_loggers` (term() default: None)
  - `mm_registry` (term() default: <vllm.multimodal.registry.MultiModalRegistry object at 0x785889a5e3c0>)
  - `use_cached_outputs` (boolean() default: False)
  - `multiprocess_mode` (boolean() default: False)
  """
  @spec new(term(), term(), boolean(), list(term()), keyword()) ::
          {:ok, SnakeBridge.Ref.t()} | {:error, Snakepit.Error.t()}
  def new(vllm_config, executor_class, log_stats, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_class(
      __MODULE__,
      :__init__,
      [vllm_config, executor_class, log_stats] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Remove request_ids from EngineCore and Detokenizer.

  ## Parameters

  - `request_ids` (list(String.t()))
  - `internal` (boolean() default: False)

  ## Returns

  - `nil`
  """
  @spec abort_request(SnakeBridge.Ref.t(), list(String.t()), list(term()), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def abort_request(ref, request_ids, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :abort_request, [request_ids] ++ List.wrap(args), opts)
  end

  @doc """
  Load a new LoRA adapter into the engine for future requests.

  ## Parameters

  - `lora_request` (term())

  ## Returns

  - `boolean()`
  """
  @spec add_lora(SnakeBridge.Ref.t(), term(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def add_lora(ref, lora_request, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :add_lora, [lora_request], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `request_id` (String.t())
  - `prompt` (term())
  - `params` (term())
  - `arrival_time` (term() default: None)
  - `lora_request` (term() default: None)
  - `tokenization_kwargs` (term() default: None)
  - `trace_headers` (term() default: None)
  - `priority` (integer() default: 0)
  - `prompt_text` (term() default: None)

  ## Returns

  - `nil`
  """
  @spec add_request(SnakeBridge.Ref.t(), String.t(), term(), term(), list(term()), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def add_request(ref, request_id, prompt, params, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :add_request,
      [request_id, prompt, params] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `func` (term())

  ## Returns

  - `list(term())`
  """
  @spec apply_model(SnakeBridge.Ref.t(), term(), keyword()) ::
          {:ok, list(term())} | {:error, Snakepit.Error.t()}
  def apply_model(ref, func, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :apply_model, [func], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `method` (term())
  - `timeout` (term() default: None)
  - `args` (tuple() default: ())
  - `kwargs` (term() default: None)

  ## Returns

  - `list(term())`
  """
  @spec collective_rpc(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, list(term())} | {:error, Snakepit.Error.t()}
  def collective_rpc(ref, method, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :collective_rpc, [method] ++ List.wrap(args), opts)
  end

  @doc """
  Log stats if logging is enabled.

  ## Returns

  - `nil`
  """
  @spec do_log_stats(SnakeBridge.Ref.t(), keyword()) :: {:ok, nil} | {:error, Snakepit.Error.t()}
  def do_log_stats(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :do_log_stats, [], opts)
  end

  @doc """
  Log stats when the time interval has passed.

  ## Returns

  - `nil`
  """
  @spec do_log_stats_with_interval(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def do_log_stats_with_interval(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :do_log_stats_with_interval, [], opts)
  end

  @doc """
  Creates an LLM engine from the engine arguments.

  ## Parameters

  - `engine_args` (term())
  - `usage_context` (term() default: <UsageContext.ENGINE_CONTEXT: 'ENGINE_CONTEXT'>)
  - `stat_loggers` (term() default: None)
  - `enable_multiprocessing` (boolean() default: False)

  ## Returns

  - `term()`
  """
  @spec from_engine_args(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def from_engine_args(ref, engine_args, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :from_engine_args,
      [engine_args] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `vllm_config` (term())
  - `usage_context` (term() default: <UsageContext.ENGINE_CONTEXT: 'ENGINE_CONTEXT'>)
  - `stat_loggers` (term() default: None)
  - `disable_log_stats` (boolean() default: False)

  ## Returns

  - `term()`
  """
  @spec from_vllm_config(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def from_vllm_config(ref, vllm_config, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :from_vllm_config,
      [vllm_config] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `list(term())`
  """
  @spec get_metrics(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, list(term())} | {:error, Snakepit.Error.t()}
  def get_metrics(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :get_metrics, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `integer()`
  """
  @spec get_num_unfinished_requests(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, integer()} | {:error, Snakepit.Error.t()}
  def get_num_unfinished_requests(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :get_num_unfinished_requests, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `{term(), term()}`
  """
  @spec get_supported_tasks(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, {term(), term()}} | {:error, Snakepit.Error.t()}
  def get_supported_tasks(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :get_supported_tasks, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `term()`
  """
  @spec get_tokenizer(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def get_tokenizer(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :get_tokenizer, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `boolean()`
  """
  @spec has_unfinished_requests(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def has_unfinished_requests(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :has_unfinished_requests, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `has_unfinished` (boolean())

  ## Returns

  - `boolean()`
  """
  @spec has_unfinished_requests_dp(SnakeBridge.Ref.t(), boolean(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def has_unfinished_requests_dp(ref, has_unfinished, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :has_unfinished_requests_dp, [has_unfinished], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `boolean()`
  """
  @spec is_sleeping(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def is_sleeping(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :is_sleeping, [], opts)
  end

  @doc """
  List all registered adapters.

  ## Returns

  - `MapSet.t(integer())`
  """
  @spec list_loras(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, MapSet.t(integer())} | {:error, Snakepit.Error.t()}
  def list_loras(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :list_loras, [], opts)
  end

  @doc """
  Prevent an adapter from being evicted.

  ## Parameters

  - `lora_id` (integer())

  ## Returns

  - `boolean()`
  """
  @spec pin_lora(SnakeBridge.Ref.t(), integer(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def pin_lora(ref, lora_id, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :pin_lora, [lora_id], opts)
  end

  @doc """
  Remove an already loaded LoRA adapter.

  ## Parameters

  - `lora_id` (integer())

  ## Returns

  - `boolean()`
  """
  @spec remove_lora(SnakeBridge.Ref.t(), integer(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def remove_lora(ref, lora_id, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :remove_lora, [lora_id], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `term()`
  """
  @spec reset_mm_cache(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def reset_mm_cache(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :reset_mm_cache, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `reset_running_requests` (boolean() default: False)
  - `reset_connector` (boolean() default: False)

  ## Returns

  - `boolean()`
  """
  @spec reset_prefix_cache(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def reset_prefix_cache(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :reset_prefix_cache, [] ++ List.wrap(args), opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `level` (integer() default: 1)

  ## Returns

  - `term()`
  """
  @spec sleep(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def sleep(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :sleep, [] ++ List.wrap(args), opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `term()`
  """
  @spec start_profile(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def start_profile(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :start_profile, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `list(term())`
  """
  @spec step(SnakeBridge.Ref.t(), keyword()) :: {:ok, list(term())} | {:error, Snakepit.Error.t()}
  def step(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :step, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `term()`
  """
  @spec stop_profile(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def stop_profile(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :stop_profile, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `outputs` (term())
  - `output_type` (term())

  ## Returns

  - `term()`
  """
  @spec validate_outputs(SnakeBridge.Ref.t(), term(), term(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def validate_outputs(ref, outputs, output_type, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :validate_outputs, [outputs, output_type], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `tags` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec wake_up(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def wake_up(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :wake_up, [] ++ List.wrap(args), opts)
  end

  @spec tokenizer(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def tokenizer(ref) do
    SnakeBridge.Runtime.get_attr(ref, :tokenizer)
  end
end
