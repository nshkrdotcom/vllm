# Generated by SnakeBridge v0.15.0 - DO NOT EDIT MANUALLY
# Regenerate with: mix compile
# Library: vllm 0.14.0
# Python module: vllm.distributed
# Python class: GroupCoordinator

defmodule Vllm.Distributed.GroupCoordinator do
  @moduledoc """
  PyTorch ProcessGroup wrapper for a group of processes.

  PyTorch ProcessGroup is bound to one specific communication backend,
      e.g. NCCL, Gloo, MPI, etc.
  GroupCoordinator takes charge of all the communication operations among
      the processes in the group. It manages both CPU and device
      communication.
  """
  def __snakebridge_python_name__, do: "vllm.distributed"
  def __snakebridge_python_class__, do: "GroupCoordinator"
  def __snakebridge_library__, do: "vllm"
  @opaque t :: SnakeBridge.Ref.t()

  @doc """
  Initialize self.  See help(type(self)) for accurate signature.

  ## Parameters

  - `group_ranks` (list(list(integer())))
  - `local_rank` (integer())
  - `torch_distributed_backend` (term())
  - `use_device_communicator` (boolean())
  - `use_message_queue_broadcaster` (boolean() default: False)
  - `group_name` (term() default: None)
  """
  @spec new(list(list(integer())), integer(), term(), boolean(), list(term()), keyword()) ::
          {:ok, SnakeBridge.Ref.t()} | {:error, Snakepit.Error.t()}
  def new(
        group_ranks,
        local_rank,
        torch_distributed_backend,
        use_device_communicator,
        args,
        opts \\ []
      ) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_class(
      __MODULE__,
      :__init__,
      [group_ranks, local_rank, torch_distributed_backend, use_device_communicator] ++
        List.wrap(args),
      opts
    )
  end

  @doc """
  Python method `GroupCoordinator._all_gather_out_place`.

  ## Parameters

  - `input_` (term())
  - `dim` (integer())

  ## Returns

  - `term()`
  """
  @spec _all_gather_out_place(SnakeBridge.Ref.t(), term(), integer(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def _all_gather_out_place(ref, input_, dim, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :_all_gather_out_place, [input_, dim], opts)
  end

  @doc """
  Python method `GroupCoordinator._all_reduce_out_place`.

  ## Parameters

  - `input_` (term())

  ## Returns

  - `term()`
  """
  @spec _all_reduce_out_place(SnakeBridge.Ref.t(), term(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def _all_reduce_out_place(ref, input_, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :_all_reduce_out_place, [input_], opts)
  end

  @doc """
  Python method `GroupCoordinator._reduce_scatter_out_place`.

  ## Parameters

  - `input_` (term())
  - `dim` (integer())

  ## Returns

  - `term()`
  """
  @spec _reduce_scatter_out_place(SnakeBridge.Ref.t(), term(), integer(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def _reduce_scatter_out_place(ref, input_, dim, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :_reduce_scatter_out_place, [input_, dim], opts)
  end

  @doc """
  Python method `GroupCoordinator.all_gather`.

  ## Parameters

  - `input_` (term())
  - `dim` (integer() default: -1)

  ## Returns

  - `term()`
  """
  @spec all_gather(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def all_gather(ref, input_, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :all_gather, [input_] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `GroupCoordinator.all_gatherv`.

  ## Parameters

  - `input_` (term())
  - `dim` (integer() default: 0)
  - `sizes` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec all_gatherv(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def all_gatherv(ref, input_, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :all_gatherv, [input_] ++ List.wrap(args), opts)
  end

  @doc """
  User-facing all-reduce function before we actually call the

  all-reduce operation.

  We need this because Dynamo does not support passing an arbitrary
  object (`self` in this case) to a custom op. We need to pass the
   group name as a string, and then look up the group coordinator from
   the group name, dispatch the all-reduce operation to the group
   coordinator.

  In addition, PyTorch custom ops do not support mutation or returning
  a new tensor in the same op. So we always make the all-reduce operation
  out-of-place.

  ## Parameters

  - `input_` (term())

  ## Returns

  - `term()`
  """
  @spec all_reduce(SnakeBridge.Ref.t(), term(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def all_reduce(ref, input_, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :all_reduce, [input_], opts)
  end

  @doc """
  Barrier synchronization among the group.

  NOTE: don't use `device_group` here! `barrier` in NCCL is
  terrible because it is internally a broadcast operation with
  secretly created GPU tensors. It is easy to mess up the current
  device. Use the CPU group instead.

  ## Returns

  - `term()`
  """
  @spec barrier(SnakeBridge.Ref.t(), keyword()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def barrier(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :barrier, [], opts)
  end

  @doc """
  Broadcast the input tensor.

  NOTE: `src` is the local rank of the source rank.

  ## Parameters

  - `input_` (term())
  - `src` (integer() default: 0)

  ## Returns

  - `term()`
  """
  @spec broadcast(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def broadcast(ref, input_, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :broadcast, [input_] ++ List.wrap(args), opts)
  end

  @doc """
  Broadcast the input object.

  NOTE: `src` is the local rank of the source rank.

  ## Parameters

  - `obj` (term() default: None)
  - `src` (integer() default: 0)

  ## Returns

  - `term()`
  """
  @spec broadcast_object(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def broadcast_object(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :broadcast_object, [] ++ List.wrap(args), opts)
  end

  @doc """
  Broadcast the input object list.

  NOTE: `src` is the local rank of the source rank.

  ## Parameters

  - `obj_list` (list(term()))
  - `src` (integer() default: 0)
  - `group` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec broadcast_object_list(SnakeBridge.Ref.t(), list(term()), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def broadcast_object_list(ref, obj_list, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :broadcast_object_list,
      [obj_list] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Broadcast the input tensor dictionary.

  NOTE: `src` is the local rank of the source rank.

  ## Parameters

  - `tensor_dict` (term() default: None)
  - `src` (integer() default: 0)
  - `group` (term() default: None)
  - `metadata_group` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec broadcast_tensor_dict(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def broadcast_tensor_dict(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :broadcast_tensor_dict, [] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `GroupCoordinator.combine`.

  ## Parameters

  - `hidden_states` (term())
  - `is_sequence_parallel` (boolean() default: False)

  ## Returns

  - `term()`
  """
  @spec combine(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def combine(ref, hidden_states, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :combine, [hidden_states] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `GroupCoordinator.create_mq_broadcaster`.

  ## Parameters

  - `writer_rank` (term() default: 0)
  - `external_writer_handle` (term() default: None)
  - `blocking` (term() default: True)

  ## Returns

  - `term()`
  """
  @spec create_mq_broadcaster(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def create_mq_broadcaster(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :create_mq_broadcaster, [] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `GroupCoordinator.create_single_reader_mq_broadcasters`.

  ## Parameters

  - `reader_rank_in_group` (term() default: 0)
  - `blocking` (term() default: False)

  ## Returns

  - `term()`
  """
  @spec create_single_reader_mq_broadcasters(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def create_single_reader_mq_broadcasters(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :create_single_reader_mq_broadcasters,
      [] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Python method `GroupCoordinator.destroy`.

  ## Returns

  - `term()`
  """
  @spec destroy(SnakeBridge.Ref.t(), keyword()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def destroy(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :destroy, [], opts)
  end

  @doc """
  Python method `GroupCoordinator.dispatch`.

  ## Parameters

  - `hidden_states` (term())
  - `router_logits` (term())
  - `is_sequence_parallel` (boolean() default: False)
  - `extra_tensors` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec dispatch(SnakeBridge.Ref.t(), term(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def dispatch(ref, hidden_states, router_logits, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :dispatch,
      [hidden_states, router_logits] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  NOTE: We assume that the input tensor is on the same device across

  all the ranks.
  NOTE: `dst` is the local rank of the destination rank.

  ## Parameters

  - `input_` (term())
  - `dst` (integer() default: 0)
  - `dim` (integer() default: -1)

  ## Returns

  - `term()`
  """
  @spec gather(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def gather(ref, input_, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :gather, [input_] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `GroupCoordinator.graph_capture`.

  ## Parameters

  - `graph_capture_context` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec graph_capture(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def graph_capture(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :graph_capture, [] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `GroupCoordinator.prepare_communication_buffer_for_model`.

  ## Parameters

  - `model` (term())

  ## Returns

  - `term()`
  """
  @spec prepare_communication_buffer_for_model(SnakeBridge.Ref.t(), term(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def prepare_communication_buffer_for_model(ref, model, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :prepare_communication_buffer_for_model, [model], opts)
  end

  @doc """
  Receives a tensor from the source rank.

  ## Parameters

  - `size` (term())
  - `dtype` (term())
  - `src` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec recv(SnakeBridge.Ref.t(), term(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def recv(ref, size, dtype, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :recv, [size, dtype] ++ List.wrap(args), opts)
  end

  @doc """
  Receive the input object list from the source rank.

  ## Parameters

  - `src` (integer())

  ## Returns

  - `term()`
  """
  @spec recv_object(SnakeBridge.Ref.t(), integer(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def recv_object(ref, src, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :recv_object, [src], opts)
  end

  @doc """
  Recv the input tensor dictionary.

  NOTE: `src` is the local rank of the source rank.

  all_gather_group: The group for the all-gather operation. If provided,
      an optimization is enabled where each rank in the group sends a
      slice of a tensor and the receiver reconstructs it using an
      all-gather, which can improve performance. This is typically the
      tensor-parallel group.
  all_gather_tensors: A dictionary to specify which tensors should use
      the all-gather optimization, which is only effective when
      `all_gather_group` is provided. By default, this optimization is
      on for any tensor whose size is divisible by the
      `all_gather_group`'s world size. However, it should be disabled
      for tensors that are not fully replicated across the group (e.g.,
      the residual tensor when sequence parallelism is enabled). This
      dictionary allows overriding the default behavior on a per-tensor
      basis.

  ## Parameters

  - `src` (term() default: None)
  - `all_gather_group` (term() | nil default: None)
  - `all_gather_tensors` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec recv_tensor_dict(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def recv_tensor_dict(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :recv_tensor_dict, [] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `GroupCoordinator.reduce_scatter`.

  ## Parameters

  - `input_` (term())
  - `dim` (integer() default: -1)

  ## Returns

  - `term()`
  """
  @spec reduce_scatter(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def reduce_scatter(ref, input_, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :reduce_scatter, [input_] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `GroupCoordinator.reduce_scatterv`.

  ## Parameters

  - `input_` (term())
  - `dim` (integer() default: -1)
  - `sizes` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec reduce_scatterv(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def reduce_scatterv(ref, input_, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :reduce_scatterv, [input_] ++ List.wrap(args), opts)
  end

  @doc """
  Sends a tensor to the destination rank in a blocking way

  ## Parameters

  - `tensor` (term())
  - `dst` (term() default: None)

  ## Returns

  - `nil`
  """
  @spec send(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def send(ref, tensor, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :send, [tensor] ++ List.wrap(args), opts)
  end

  @doc """
  Send the input object list to the destination rank.

  ## Parameters

  - `obj` (term())
  - `dst` (integer())

  ## Returns

  - `nil`
  """
  @spec send_object(SnakeBridge.Ref.t(), term(), integer(), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def send_object(ref, obj, dst, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :send_object, [obj, dst], opts)
  end

  @doc """
  Send the input tensor dictionary.

  NOTE: `dst` is the local rank of the source rank.

  all_gather_group: The group for the all-gather operation. If provided,
      an optimization is enabled where each rank in the group sends a
      slice of a tensor and the receiver reconstructs it using an
      all-gather, which can improve performance. This is typically the
      tensor-parallel group.
  all_gather_tensors: A dictionary to specify which tensors should use
      the all-gather optimization, which is only effective when
      `all_gather_group` is provided. By default, this optimization is
      on for any tensor whose size is divisible by the
      `all_gather_group`'s world size. However, it should be disabled
      for tensors that are not fully replicated across the group (e.g.,
      the residual tensor when sequence parallelism is enabled). This
      dictionary allows overriding the default behavior on a per-tensor
      basis.

  ## Parameters

  - `tensor_dict` (%{optional(String.t()) => term()})
  - `dst` (term() default: None)
  - `all_gather_group` (term() | nil default: None)
  - `all_gather_tensors` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec send_tensor_dict(
          SnakeBridge.Ref.t(),
          %{optional(String.t()) => term()},
          list(term()),
          keyword()
        ) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def send_tensor_dict(ref, tensor_dict, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :send_tensor_dict,
      [tensor_dict] ++ List.wrap(args),
      opts
    )
  end

  @spec first_rank(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def first_rank(ref) do
    SnakeBridge.Runtime.get_attr(ref, :first_rank)
  end

  @spec is_first_rank(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def is_first_rank(ref) do
    SnakeBridge.Runtime.get_attr(ref, :is_first_rank)
  end

  @spec is_last_rank(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def is_last_rank(ref) do
    SnakeBridge.Runtime.get_attr(ref, :is_last_rank)
  end

  @spec last_rank(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def last_rank(ref) do
    SnakeBridge.Runtime.get_attr(ref, :last_rank)
  end

  @spec next_rank(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def next_rank(ref) do
    SnakeBridge.Runtime.get_attr(ref, :next_rank)
  end

  @spec prev_rank(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def prev_rank(ref) do
    SnakeBridge.Runtime.get_attr(ref, :prev_rank)
  end
end
