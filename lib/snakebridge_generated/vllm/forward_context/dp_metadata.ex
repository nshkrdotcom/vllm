# Generated by SnakeBridge v0.15.0 - DO NOT EDIT MANUALLY
# Regenerate with: mix compile
# Library: vllm 0.14.0
# Python module: vllm.forward_context
# Python class: DPMetadata

defmodule Vllm.ForwardContext.DPMetadata do
  @moduledoc """
  DPMetadata(max_tokens_across_dp_cpu: torch.Tensor, num_tokens_across_dp_cpu: torch.Tensor, local_sizes: list[int] | None = None)
  """
  def __snakebridge_python_name__, do: "vllm.forward_context"
  def __snakebridge_python_class__, do: "DPMetadata"
  def __snakebridge_library__, do: "vllm"
  @opaque t :: SnakeBridge.Ref.t()

  @doc """
  Initialize self.  See help(type(self)) for accurate signature.

  ## Parameters

  - `max_tokens_across_dp_cpu` (term())
  - `num_tokens_across_dp_cpu` (term())
  - `local_sizes` (term() default: None)
  """
  @spec new(term(), term(), list(term()), keyword()) ::
          {:ok, SnakeBridge.Ref.t()} | {:error, Snakepit.Error.t()}
  def new(max_tokens_across_dp_cpu, num_tokens_across_dp_cpu, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_class(
      __MODULE__,
      :__init__,
      [max_tokens_across_dp_cpu, num_tokens_across_dp_cpu] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Context manager to compute and temporarily set the per-rank local token

  sizes for a specific chunk during chunked forward execution.

  This is necessary to ensure each DP (data parallel) rank processes its
  designated portion of tokens in lockstep with others, even when the
  token counts are uneven or some ranks have completed their input early.

  For chunked execution, we break up the total tokens on each rank into
  multiple chunks (of at most `max_chunk_size_per_rank`), and for a given
  `chunk_idx`, this context manager sets `self.local_sizes` to the number
  of tokens to process in that chunk on each rank.

  `self.local_sizes` is only valid inside the context.

  ## Parameters

  - `sequence_parallel_size` - When Attn is TP and MoE layers are EP, we use SP between the layers to avoid redundant ops. We need this value to compute the chunked sizes.
  - `max_chunk_size_per_rank` - The max number of tokens each rank is allowed to process in this chunk.
  - `chunk_idx` - The index of the chunk to compute sizes for.

  ## Returns

  - `term()`
  """
  @spec chunked_sizes(SnakeBridge.Ref.t(), integer(), integer(), integer(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def chunked_sizes(ref, sequence_parallel_size, max_chunk_size_per_rank, chunk_idx, opts \\ []) do
    SnakeBridge.Runtime.call_method(
      ref,
      :chunked_sizes,
      [sequence_parallel_size, max_chunk_size_per_rank, chunk_idx],
      opts
    )
  end

  @doc """
  Python method `DPMetadata.cu_tokens_across_sp`.

  ## Parameters

  - `sp_size` (integer())

  ## Returns

  - `term()`
  """
  @spec cu_tokens_across_sp(SnakeBridge.Ref.t(), integer(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def cu_tokens_across_sp(ref, sp_size, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :cu_tokens_across_sp, [sp_size], opts)
  end

  @doc """
  Python method `DPMetadata.get_chunk_sizes_across_dp_rank`.

  ## Returns

  - `term()`
  """
  @spec get_chunk_sizes_across_dp_rank(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def get_chunk_sizes_across_dp_rank(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :get_chunk_sizes_across_dp_rank, [], opts)
  end

  @doc """
  Python method `DPMetadata.make`.

  ## Parameters

  - `parallel_config` (term())
  - `num_tokens` (integer())
  - `num_tokens_across_dp_cpu` (term())

  ## Returns

  - `Vllm.ForwardContext.DPMetadata.t()`
  """
  @spec make(SnakeBridge.Ref.t(), term(), integer(), term(), keyword()) ::
          {:ok, Vllm.ForwardContext.DPMetadata.t()} | {:error, Snakepit.Error.t()}
  def make(ref, parallel_config, num_tokens, num_tokens_across_dp_cpu, opts \\ []) do
    SnakeBridge.Runtime.call_method(
      ref,
      :make,
      [parallel_config, num_tokens, num_tokens_across_dp_cpu],
      opts
    )
  end

  @doc """
  Context manager for setting self.local_sizes. Same as self.chunked_sizes

  but without any chunking.

  ## Parameters

  - `sequence_parallel_size` (integer())

  ## Returns

  - `term()`
  """
  @spec sp_local_sizes(SnakeBridge.Ref.t(), integer(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def sp_local_sizes(ref, sequence_parallel_size, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :sp_local_sizes, [sequence_parallel_size], opts)
  end

  @spec local_sizes(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def local_sizes(ref) do
    SnakeBridge.Runtime.get_attr(ref, :local_sizes)
  end
end
