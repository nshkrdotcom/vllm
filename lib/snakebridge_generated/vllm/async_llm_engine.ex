# Generated by SnakeBridge v0.15.0 - DO NOT EDIT MANUALLY
# Regenerate with: mix compile
# Library: vllm 0.14.0
# Python module: vllm
# Python class: AsyncLLMEngine

defmodule Vllm.AsyncLLMEngine do
  @moduledoc """
  Protocol class for Clients to Engine
  """
  def __snakebridge_python_name__, do: "vllm"
  def __snakebridge_python_class__, do: "AsyncLLMEngine"
  def __snakebridge_library__, do: "vllm"
  @opaque t :: SnakeBridge.Ref.t()

  @doc """
  Create an AsyncLLM.



  ## Parameters

  - `vllm_config` - global configuration.
  - `executor_class` - an Executor impl, e.g. MultiprocExecutor.
  - `log_stats` - Whether to log stats.
  - `usage_context` - Usage context of the LLM.
  - `mm_registry` - Multi-modal registry.
  - `use_cached_outputs` - Whether to use cached outputs.
  - `log_requests` - Whether to log requests.
  - `start_engine_loop` - Whether to start the engine loop.
  - `stat_loggers` - customized stat loggers for the engine. If not provided, default stat loggers will be used. PLEASE BE AWARE THAT STAT LOGGER IS NOT STABLE IN V1, AND ITS BASE CLASS INTERFACE MIGHT CHANGE.
  """
  @spec new(term(), term(), boolean(), list(term()), keyword()) ::
          {:ok, SnakeBridge.Ref.t()} | {:error, Snakepit.Error.t()}
  def new(vllm_config, executor_class, log_stats, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_class(
      __MODULE__,
      :__init__,
      [vllm_config, executor_class, log_stats] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `request` (term())
  - `prompt` (term())
  - `parent_req` (term())
  - `index` (integer())
  - `queue` (term())

  ## Returns

  - `term()`
  """
  @spec _add_request(SnakeBridge.Ref.t(), term(), term(), term(), integer(), term(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def _add_request(ref, request, prompt, parent_req, index, queue, opts \\ []) do
    SnakeBridge.Runtime.call_method(
      ref,
      :_add_request,
      [request, prompt, parent_req, index, queue],
      opts
    )
  end

  @doc """
  Background loop: pulls from EngineCore and pushes to AsyncStreams.

  ## Returns

  - `term()`
  """
  @spec _run_output_handler(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def _run_output_handler(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :_run_output_handler, [], opts)
  end

  @doc """
  Abort RequestId in OutputProcessor and EngineCore.

  ## Parameters

  - `request_id` (term())
  - `internal` (boolean() default: False)

  ## Returns

  - `nil`
  """
  @spec abort(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def abort(ref, request_id, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :abort, [request_id] ++ List.wrap(args), opts)
  end

  @doc """
  Load a new LoRA adapter into the engine for future requests.

  ## Parameters

  - `lora_request` (term())

  ## Returns

  - `boolean()`
  """
  @spec add_lora(SnakeBridge.Ref.t(), term(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def add_lora(ref, lora_request, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :add_lora, [lora_request], opts)
  end

  @doc """
  Add new request to the AsyncLLM.

  ## Parameters

  - `request_id` (String.t())
  - `prompt` (term())
  - `params` (term())
  - `arrival_time` (term() default: None)
  - `lora_request` (term() default: None)
  - `tokenization_kwargs` (term() default: None)
  - `trace_headers` (term() default: None)
  - `priority` (integer() default: 0)
  - `data_parallel_rank` (term() default: None)
  - `prompt_text` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec add_request(SnakeBridge.Ref.t(), String.t(), term(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def add_request(ref, request_id, prompt, params, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :add_request,
      [request_id, prompt, params] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Raise if unhealthy

  ## Returns

  - `nil`
  """
  @spec check_health(SnakeBridge.Ref.t(), keyword()) :: {:ok, nil} | {:error, Snakepit.Error.t()}
  def check_health(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :check_health, [], opts)
  end

  @doc """
  Perform a collective RPC call to the given path.

  ## Parameters

  - `method` (String.t())
  - `timeout` (term() default: None)
  - `args` (tuple() default: ())
  - `kwargs` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec collective_rpc(SnakeBridge.Ref.t(), String.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def collective_rpc(ref, method, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :collective_rpc, [method] ++ List.wrap(args), opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `nil`
  """
  @spec do_log_stats(SnakeBridge.Ref.t(), keyword()) :: {:ok, nil} | {:error, Snakepit.Error.t()}
  def do_log_stats(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :do_log_stats, [], opts)
  end

  @doc """
  Main function called by the API server to kick off a request

  * 1) Making an AsyncStream corresponding to the Request.
      * 2) Processing the Input.
      * 3) Adding the Request to the EngineCore (separate process).

  A separate output_handler loop runs in a background AsyncIO task,
  pulling outputs from EngineCore and putting them into the
  per-request AsyncStream.

  The caller of generate() iterates the returned AsyncGenerator,
  returning the RequestOutput back to the caller.

  NOTE: truncate_prompt_tokens is deprecated in v0.14.
  TODO: Remove truncate_prompt_tokens in v0.15.

  ## Parameters

  - `prompt` (term())
  - `pooling_params` (Vllm.PoolingParamsClass.t())
  - `request_id` (String.t())
  - `lora_request` (term() default: None)
  - `trace_headers` (term() default: None)
  - `priority` (integer() default: 0)
  - `truncate_prompt_tokens` (term() default: None)
  - `tokenization_kwargs` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec encode(
          SnakeBridge.Ref.t(),
          term(),
          Vllm.PoolingParamsClass.t(),
          String.t(),
          list(term()),
          keyword()
        ) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def encode(ref, prompt, pooling_params, request_id, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :encode,
      [prompt, pooling_params, request_id] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Create an AsyncLLM from the EngineArgs.

  ## Parameters

  - `engine_args` (term())
  - `start_engine_loop` (boolean() default: True)
  - `usage_context` (term() default: <UsageContext.ENGINE_CONTEXT: 'ENGINE_CONTEXT'>)
  - `stat_loggers` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec from_engine_args(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def from_engine_args(ref, engine_args, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :from_engine_args,
      [engine_args] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Parameters

  - `vllm_config` (term())
  - `start_engine_loop` (boolean() default: True)
  - `usage_context` (term() default: <UsageContext.ENGINE_CONTEXT: 'ENGINE_CONTEXT'>)
  - `stat_loggers` (term() default: None)
  - `enable_log_requests` (boolean() default: False)
  - `aggregate_engine_logging` (boolean() default: False)
  - `disable_log_stats` (boolean() default: False)
  - `client_addresses` (term() default: None)
  - `client_count` (integer() default: 1)
  - `client_index` (integer() default: 0)

  ## Returns

  - `term()`
  """
  @spec from_vllm_config(SnakeBridge.Ref.t(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def from_vllm_config(ref, vllm_config, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :from_vllm_config,
      [vllm_config] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Main function called by the API server to kick off a request

  * 1) Making an AsyncStream corresponding to the Request.
      * 2) Processing the Input.
      * 3) Adding the Request to the Detokenizer.
      * 4) Adding the Request to the EngineCore (separate process).

  A separate output_handler loop runs in a background AsyncIO task,
  pulling outputs from EngineCore and putting them into the
  per-request AsyncStream.

  The caller of generate() iterates the returned AsyncGenerator,
  returning the RequestOutput back to the caller.

  ## Parameters

  - `prompt` (term())
  - `sampling_params` (Vllm.SamplingParamsClass.t())
  - `request_id` (String.t())
  - `prompt_text` (term() keyword-only default: None)
  - `lora_request` (term() keyword-only default: None)
  - `tokenization_kwargs` (term() keyword-only default: None)
  - `trace_headers` (term() keyword-only default: None)
  - `priority` (integer() keyword-only default: 0)
  - `data_parallel_rank` (term() keyword-only default: None)

  ## Returns

  - `term()`
  """
  @spec generate(SnakeBridge.Ref.t(), term(), Vllm.SamplingParamsClass.t(), String.t(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def generate(ref, prompt, sampling_params, request_id, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :generate, [prompt, sampling_params, request_id], opts)
  end

  @doc """
  Get supported tasks

  ## Returns

  - `{term(), term()}`
  """
  @spec get_supported_tasks(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, {term(), term()}} | {:error, Snakepit.Error.t()}
  def get_supported_tasks(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :get_supported_tasks, [], opts)
  end

  @doc """
  Get the tokenizer

  ## Returns

  - `term()`
  """
  @spec get_tokenizer(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def get_tokenizer(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :get_tokenizer, [], opts)
  end

  @doc """
  Return whether the engine is currently paused.

  ## Returns

  - `boolean()`
  """
  @spec is_paused(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def is_paused(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :is_paused, [], opts)
  end

  @doc """
  Check whether the engine is sleeping

  ## Returns

  - `boolean()`
  """
  @spec is_sleeping(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def is_sleeping(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :is_sleeping, [], opts)
  end

  @doc """
  vLLM: a high-throughput and memory-efficient inference engine for LLMs

  ## Returns

  - `boolean()`
  """
  @spec is_tracing_enabled(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def is_tracing_enabled(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :is_tracing_enabled, [], opts)
  end

  @doc """
  List all registered adapters.

  ## Returns

  - `MapSet.t(integer())`
  """
  @spec list_loras(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, MapSet.t(integer())} | {:error, Snakepit.Error.t()}
  def list_loras(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :list_loras, [], opts)
  end

  @doc """
  Pause generation to allow model weight updates.

  New generation/encoding requests are blocked until resume.

  ## Parameters

  - `wait_for_inflight_requests` - When ``True`` waits for in-flight requests to finish before pausing. When ``False`` (default), immediately aborts any in-flight requests.
  - `clear_cache` - Whether to clear KV cache and prefix cache after draining. Set to ``False`` to preserve cache for faster resume. Default is ``True`` (clear caches).

  ## Returns

  - `nil`
  """
  @spec pause_generation(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def pause_generation(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :pause_generation, [], opts)
  end

  @doc """
  Prevent an adapter from being evicted.

  ## Parameters

  - `lora_id` (integer())

  ## Returns

  - `boolean()`
  """
  @spec pin_lora(SnakeBridge.Ref.t(), integer(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def pin_lora(ref, lora_id, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :pin_lora, [lora_id], opts)
  end

  @doc """
  Remove an already loaded LoRA adapter.

  ## Parameters

  - `lora_id` (integer())

  ## Returns

  - `boolean()`
  """
  @spec remove_lora(SnakeBridge.Ref.t(), integer(), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def remove_lora(ref, lora_id, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :remove_lora, [lora_id], opts)
  end

  @doc """
  Reset the multi-modal cache

  ## Returns

  - `nil`
  """
  @spec reset_mm_cache(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def reset_mm_cache(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :reset_mm_cache, [], opts)
  end

  @doc """
  Reset the prefix cache and optionally any configured connector cache

  ## Parameters

  - `reset_running_requests` (boolean() default: False)
  - `reset_connector` (boolean() default: False)

  ## Returns

  - `boolean()`
  """
  @spec reset_prefix_cache(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, boolean()} | {:error, Snakepit.Error.t()}
  def reset_prefix_cache(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :reset_prefix_cache, [] ++ List.wrap(args), opts)
  end

  @doc """
  Resume generation after :meth:`pause_generation`.

  ## Returns

  - `nil`
  """
  @spec resume_generation(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def resume_generation(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :resume_generation, [], opts)
  end

  @doc """
  Scale up or down the data parallel size by adding or removing

  engine cores.

  ## Parameters

  - `new_data_parallel_size` - The new number of data parallel workers
  - `drain_timeout` - Maximum time to wait for requests to drain (seconds)

  ## Returns

  - `term()`
  """
  @spec scale_elastic_ep(SnakeBridge.Ref.t(), integer(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def scale_elastic_ep(ref, new_data_parallel_size, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :scale_elastic_ep,
      [new_data_parallel_size] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Shutdown, cleaning up the background proc and IPC.

  ## Returns

  - `term()`
  """
  @spec shutdown(SnakeBridge.Ref.t(), keyword()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def shutdown(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :shutdown, [], opts)
  end

  @doc """
  Sleep the engine

  ## Parameters

  - `level` (integer() default: 1)

  ## Returns

  - `nil`
  """
  @spec sleep(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def sleep(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :sleep, [] ++ List.wrap(args), opts)
  end

  @doc """
  Start profiling the engine

  ## Returns

  - `nil`
  """
  @spec start_profile(SnakeBridge.Ref.t(), keyword()) :: {:ok, nil} | {:error, Snakepit.Error.t()}
  def start_profile(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :start_profile, [], opts)
  end

  @doc """
  Stop profiling the engine

  ## Returns

  - `nil`
  """
  @spec stop_profile(SnakeBridge.Ref.t(), keyword()) :: {:ok, nil} | {:error, Snakepit.Error.t()}
  def stop_profile(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :stop_profile, [], opts)
  end

  @doc """
  Wait for all requests to be drained.

  ## Parameters

  - `drain_timeout` (integer() default: 300)

  ## Returns

  - `term()`
  """
  @spec wait_for_requests_to_drain(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def wait_for_requests_to_drain(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :wait_for_requests_to_drain, [] ++ List.wrap(args), opts)
  end

  @doc """
  Wake up the engine

  ## Parameters

  - `tags` (term() default: None)

  ## Returns

  - `nil`
  """
  @spec wake_up(SnakeBridge.Ref.t(), list(term()), keyword()) ::
          {:ok, nil} | {:error, Snakepit.Error.t()}
  def wake_up(ref, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :wake_up, [] ++ List.wrap(args), opts)
  end

  @spec _abc_impl(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def _abc_impl(ref) do
    SnakeBridge.Runtime.get_attr(ref, :_abc_impl)
  end

  @spec dead_error(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def dead_error(ref) do
    SnakeBridge.Runtime.get_attr(ref, :dead_error)
  end

  @spec errored(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def errored(ref) do
    SnakeBridge.Runtime.get_attr(ref, :errored)
  end

  @spec is_running(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def is_running(ref) do
    SnakeBridge.Runtime.get_attr(ref, :is_running)
  end

  @spec is_stopped(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def is_stopped(ref) do
    SnakeBridge.Runtime.get_attr(ref, :is_stopped)
  end

  @spec tokenizer(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def tokenizer(ref) do
    SnakeBridge.Runtime.get_attr(ref, :tokenizer)
  end
end
